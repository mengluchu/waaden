{"cells":[{"metadata":{},"cell_type":"markdown","source":"import packages"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom osgeo import gdal_array\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport glob\nfrom torch import Tensor, einsum\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom scipy.ndimage import distance_transform_edt as distance\nfrom scipy.spatial.distance import directed_hausdorff\nfrom typing import Any, Callable, Iterable, List, Set, Tuple, TypeVar, Union\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install segmentation-models-pytorch==0.1.0\n!pip install keras-unet\n\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import segmentation_models_pytorch as smp\nfrom keras_unet.utils import plot_imgs\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"size = 256\nbs = 16 #batchsize\nclasses = [ 1, 2, 4,  5,  6,  7,  8,  9, 10, 15]\nnum_class= len(classes)\nEPOCH= 200\nreduce = 4","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Used to crop the imput images"},{"metadata":{"trusted":true},"cell_type":"code","source":"def slice (arr, size, inputsize,stride):\n    result = []\n    if stride is None:\n        stride = size\n    for i in range(0, (inputsize-size)+1, stride):\n        for j in range(0, (inputsize-size)+1, stride):\n        \n            if arr.ndim == 3:\n                s = arr[:,i:(i+size),j:(j+size), ]\n            else:\n                s = arr[i:(i+size),j:(j+size), ]\n            result.append(s)\n            #print(i,\"\",j)\n    result = np.array(result)\n    return result\n\ndef batchslice (arr, size, inputsize, stride, num_img):\n    result = []\n    for i in range(0, num_img):\n        s= slice(arr[i,], size, inputsize, stride )\n        result.append(s )\n    result = np.array(result)\n    result = result.reshape(result.shape[0]*result.shape[1], result.shape[2], result.shape[3], -1)\n    return result\n\ndef class2dim (mask, CLASSES):\n    \n        masks = [(mask == v) for v in CLASSES ]\n        mask = np.stack(masks, axis=-1).astype('float')    \n        return mask\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"load the different tiles into 1 variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#stack all files into 1 variable\ndef load_raster(path,data,tile1,tile2,reduce):\n    tiles_in=[tile1,tile2]\n    files = []\n    for tile in tiles_in:\n       for file in glob.glob(path+data+'*{}.tif'.format(tile)):\n           file1 = gdal_array.LoadFile(file)\n           #only use 50% of the points to reduce memory\n           if np.ndim(file1)==3:\n               file1=file1[:,::reduce,::reduce]\n           else:\n               file1=file1[::reduce,::reduce]\n           files.append(file1)\n    stacked = np.array(files)\n    return stacked\n\ndef load_data(path,data1,data2,tile1,tile2, reduce):\n    part1 = load_raster(path,data1,tile1,tile2, reduce )\n    part2 = load_raster(path,data2,tile1,tile2,reduce )\n    \n    if np.ndim(part1)< np.ndim(part2):#check if dimmensions are equal\n       part1 = np.expand_dims(part1,axis=1)\n    elif np.ndim(part1)> np.ndim(part2):\n       part2 = np.expand_dims(part2,axis=1)\n    print(part1.shape,part2.shape)\n    total = np.concatenate((part1,part2),axis=1)\n    return total\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Slice input data into smaller size images"},{"metadata":{"trusted":true},"cell_type":"code","source":"training= load_data(\"../input/guided-research/Training/\",\"DEM\",\"Wadden\",\"12_7\",\"13_8\",reduce)\nvalidation = load_data(\"../input/guided-research/Validation/\",\"DEM\",\"Wadden\",\"12_8\",\"14_7\",reduce)\ny_train= load_raster(\"../input/guided-research/Training/\",\"class\",\"12_7\",\"13_8\", reduce )\ny_val=load_raster(\"../input/guided-research/Validation/\",\"class\",\"12_8\",\"14_7\",reduce )\nprint(training.shape, validation.shape, y_train.shape, y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def uniq(a: Tensor) -> Set:\n    return set(torch.unique(a.cpu()).numpy())\n\n\ndef sset(a: Tensor, sub: Iterable) -> bool:\n    return uniq(a).issubset(sub)\n\ndef simplex(t: Tensor, axis=1) -> bool:\n    _sum = t.sum(axis).type(torch.float32)\n    _ones = torch.ones_like(_sum, dtype=torch.float32)\n    return torch.allclose(_sum, _ones)\n\n\ndef one_hot(t: Tensor, axis=1) -> bool:\n    return simplex(t, axis) and sset(t, [0, 1])\n\ndef one_hot2dist(seg: np.ndarray) -> np.ndarray:\n    assert one_hot(torch.Tensor(seg), axis=0)\n    C: int = len(seg)\n\n    res = np.zeros_like(seg)\n    for c in range(C):\n        posmask = seg[c].astype(np.bool)\n\n        if posmask.any():\n            negmask = ~posmask\n            res[c] = distance(negmask) * negmask - (distance(posmask) - 1) * posmask\n    return res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n    \n\n''' \nclass GeneralizedDice():\n    def __init__(self, **kwargs):\n        # Self.idc is used to filter out some classes of the target mask. Use fancy indexing\n        self.idc: List[int] = kwargs[\"idc\"]\n        print(f\"Initialized {self.__class__.__name__} with {kwargs}\")\n\n    def __call__(self, probs: Tensor, target: Tensor, _: Tensor) -> Tensor:\n        assert simplex(probs) and simplex(target)\n\n        pc = probs[:, self.idc, ...].type(torch.float32)\n        tc = target[:, self.idc, ...].type(torch.float32)\n\n        w: Tensor = 1 / ((einsum(\"bcwh->bc\", tc).type(torch.float32) + 1e-10) ** 2)\n        intersection: Tensor = w * einsum(\"bcwh,bcwh->bc\", pc, tc)\n        union: Tensor = w * (einsum(\"bcwh->bc\", pc) + einsum(\"bcwh->bc\", tc))\n\n        divided: Tensor = 1 - 2 * (einsum(\"bc->b\", intersection) + 1e-10) / (einsum(\"bc->b\", union) + 1e-10)\n\n        loss = divided.mean()\n\n        return loss\n\nclass DiceLoss():\n    def __init__(self, **kwargs):\n        # Self.idc is used to filter out some classes of the target mask. Use fancy indexing\n        self.idc: List[int] = kwargs[\"idc\"]\n        print(f\"Initialized {self.__class__.__name__} with {kwargs}\")\n\n    def __call__(self, probs: Tensor, target: Tensor, _: Tensor) -> Tensor:\n        assert simplex(probs) and simplex(target)\n\n        pc = probs[:, self.idc, ...].type(torch.float32)\n        tc = target[:, self.idc, ...].type(torch.float32)\n\n        intersection: Tensor = einsum(\"bcwh,bcwh->bc\", pc, tc)\n        union: Tensor = (einsum(\"bcwh->bc\", pc) + einsum(\"bcwh->bc\", tc))\n\n        divided: Tensor = 1 - (2 * intersection + 1e-10) / (union + 1e-10)\n\n        loss = divided.mean()\n\n        return loss\n\n\nclass SurfaceLoss():\n    def __init__(self, **kwargs):\n        # Self.idc is used to filter out some classes of the target mask. Use fancy indexing\n        self.idc: List[int] = kwargs[\"idc\"]\n        print(f\"Initialized {self.__class__.__name__} with {kwargs}\")\n\n    def __call__(self, probs: Tensor, dist_maps: Tensor, _: Tensor) -> Tensor:\n        assert simplex(probs)\n        assert not one_hot(dist_maps)\n\n        pc = probs[:, self.idc, ...].type(torch.float32)\n        dc = dist_maps[:, self.idc, ...].type(torch.float32)\n\n        multipled = einsum(\"bcwh,bcwh->bcwh\", pc, dc)\n\n        loss = multipled.mean()\n\n        return loss\n\n'''     ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(training.shape, validation.shape,y_train.shape, y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nplot_imgs(\n    org_imgs=training[1:2,0,:,:], # required - original images\n    mask_imgs=y_train[1:2,:,:], # required - ground truth masks\n    \n    nm_img_to_plot=3) # optional - number of images to plot\n\nplot_imgs(\n    org_imgs=training[1:2,2,:,:], # required - original images\n    mask_imgs=y_train[1:2,:,:], # required - ground truth masks\n    \n    nm_img_to_plot=3) # optional - number of images to plot\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_imgs(\n    org_imgs=validation[0:1,0, :,:], # required - original images\n    mask_imgs=y_val[0:1,:,:], # required - ground truth masks\n    \n    nm_img_to_plot=3) # optional - number of images to plot\nplot_imgs(\n    org_imgs=validation[0:1,1, :,:], # required - original images\n    mask_imgs=y_val[0:1,:,:], # required - ground truth masks\n    \n    nm_img_to_plot=3) # optional - number of images to plot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#np.moveaxis(validation[0:2,[1,2,3], :,:], 1 , -1)\nplot_imgs(\n    org_imgs=validation[1:2,0, :,:], # required - original images\n    mask_imgs=y_val[1:2,:,:], # required - ground truth masks\n    \n    nm_img_to_plot=3) # optional - number of images to plot\nplot_imgs(\n    org_imgs=validation[1:2,1, :,:], # required - original images\n    mask_imgs=y_val[1:2,:,:], # required - ground truth masks\n    \n    nm_img_to_plot=3) # optional - number of images to plot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#a= np.moveaxis(validation[1:2 ,1:4,:,:], 1, -1)\n# required - original images\n \nmultival = class2dim(y_val, classes) \nfor i in range(len(classes)):\n    plot_imgs(\n    org_imgs=validation[0:1, 2, :,: ],\n    mask_imgs=multival[0:1,:,:,i], # required - original images \n\n    nm_img_to_plot=10) # optional - number of images to plot\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training  = batchslice(training,size,training[0].shape[1],size,training.shape[0])\nvalidation=batchslice(validation,size,validation[0].shape[1],size,validation.shape[0])\n\ny_train = batchslice(y_train,size,y_train[0].shape[1],size,y_train.shape[0]).squeeze()\ny_val = batchslice(y_val,size,y_val[0].shape[1],size,y_val.shape[0]).squeeze()\n  \n\nprint(training.shape,validation.shape,y_train.shape, y_val.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.itraining= load_data(\"../input/guided-research/Training/\",\"DEM\",\"Wadden\",\"12_7\",\"13_8\")mshow(validation[1,0,:,:])\n#plt.show() \nplot_imgs(\n    org_imgs=validation[0:9,1, :,:], # required - original images\n    mask_imgs=y_val[0:9,:,:], # required - ground truth masks\n    \n    nm_img_to_plot=10) # optional - number of images to plot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for wh in range(10,20):\n#    plt.imshow(validation[wh,0,:,:])\n#    plt.show() \n#    plt.imshow(validation[wh,2,:,:])\n#    plt.show() \n#    plt.imshow(y_val[wh,:,:])\n#    plt.show()  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_mask = class2dim(y_train, classes)\nnew_mas_val = class2dim(y_val, classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check\n#whichone = 5\n#print(np.amax(y_train[whichone,] ),np.amin(y_train[whichone,] ))\n#plt.imshow(y_train[whichone,] )\n#plt.show() \n#for n in range(num_class): \n#    plt.imshow(new_mask[whichone,:,:,n])     \n#    plt.show() \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Code used to see slicing result"},{"metadata":{"trusted":true},"cell_type":"code","source":"new_mask =  np.moveaxis(new_mask, -1, 1)\nnew_mas_val = np.moveaxis(new_mas_val, -1, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_mask.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.imshow(new_mas_val[0,1,])\na = one_hot2dist(new_mas_val[0,])\nplt.imshow(a[1,])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training[training> 1e308] = 0 \n#np.nan\nvalidation[validation> 1e308] =0 \nnew_mask[new_mask >1e308] = 0\nnew_mas_val[new_mas_val> 1e308] = 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"training = list(zip(training[:,1,:,:], y_train))\nvalidation = list(zip(validation[:,1,:,:], y_val))m\n "},{"metadata":{"trusted":true},"cell_type":"code","source":"#np.save(\"../wadden1\", zip(training, new_mask, validation, new_mas_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Get torch loader, got the error in catalyst with \"collection dont have attribute size\", use original array as demonstrated in catalyst"},{"metadata":{"trusted":true},"cell_type":"code","source":"#net = torch.hub.load('milesial/Pytorch-UNet', 'unet_carvana')\n#print(net)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#CLASSES = [format(x, '2d') for x in  classes]\n \n#model = smp.Unet(classes=len(CLASSES), in_channels=3,activation=\"sigmoid\")\n#newmodel = torch.nn.Sequential(*(list(model.children())[:-1]))\n#print(model)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model = net\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#comparison = training == validation\n#equal_arrays = comparison.all()\n#print(equal_arrays)\n#inputs, masks = next(iter(loaders['valid']))\n ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pre-trained Unet segmentation model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from catalyst.dl import utils\nimport collections\nfrom catalyst.data import Augmentor\nimport torchvision.transforms as transforms\n\nopen_fn = lambda x: {\"features\": x[0], \"targets\": x[1]}\ndata_transform = transforms.Compose([\n    Augmentor(\n        dict_key=\"features\",\n        augment_fn=lambda x: \\\n            torch.from_numpy(x.copy().astype(np.float32) / 1.)),\n  #  Augmentor(\n  #      dict_key=\"features\",\n  #      augment_fn=transforms.Normalize(\n  #          (0.5, ),\n  #          (0.5, ))),\n    Augmentor(\n        dict_key=\"targets\",\n        augment_fn=lambda x: \\\n            torch.from_numpy(x.copy().astype(np.float32) / 1.) )\n])\n#.unsqueeze_(0)\n\n\ntrain_loader = utils.get_loader(\n    list(zip(training.astype(np.float32), new_mask.astype(np.float32))), \n    open_fn=open_fn,  \n    batch_size=bs, \n    dict_transform=data_transform, \n    num_workers=4, \n    shuffle=True)\n\nvalid_loader = utils.get_loader(\n    list(zip(validation.astype(np.float32), new_mas_val.astype(np.float32))), \n    open_fn=open_fn,  \n    batch_size=bs, \n    dict_transform=data_transform, \n    num_workers=4, \n    shuffle=False)\n\n \nfrom catalyst.dl import utils\n \nloaders = collections.OrderedDict()\n\n\nloaders[\"train\"] = train_loader\nloaders[\"valid\"] =valid_loader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nfor i, inputs in enumerate (loaders['train']):          \n         print (inputs['features'].shape)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"%load_ext tensorboard\n%tensorboard --logdir {logdir}"},{"metadata":{"trusted":true},"cell_type":"code","source":"GAMMA = 2\nALPHA = 0.8 # emphasize FP\nBETA = 0.2 # more emphasize on FN","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DiceBCELoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceBCELoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        intersection = (inputs * targets).sum()                            \n        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n        Dice_BCE = BCE + dice_loss\n        \n        return Dice_BCE\n\n\n     \nclass IoULoss(nn.Module):\n    def __init__(self, weight=None, size_average=True, **kwargs):\n        super(IoULoss, self).__init__(**kwargs)\n\n    def forward(self, inputs, targets, smooth=1):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        #intersection is equivalent to True Positive count\n        #union is the mutually inclusive area of all labels & predictions \n        intersection = (inputs * targets).sum()\n        total = (inputs + targets).sum()\n        union = total - intersection \n        \n        IoU = (intersection + smooth)/(union + smooth)\n                \n        return 1 - IoU\n    \nclass TverskyLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(TverskyLoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1, alpha=ALPHA, beta=BETA):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        #True Positives, False Positives & False Negatives\n        TP = (inputs * targets).sum()    \n        FP = ((1-targets) * inputs).sum()\n        FN = (targets * (1-inputs)).sum()\n       \n        Tversky = (TP + smooth) / (TP + alpha*FP + beta*FN + smooth)  \n        \n        return 1 - Tversky\n \n\nclass FocalTverskyLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(FocalTverskyLoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1, alpha=ALPHA, beta=BETA, gamma=GAMMA):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        #True Positives, False Positives & False Negatives\n        TP = (inputs * targets).sum()    \n        FP = ((1-targets) * inputs).sum()\n        FN = (targets * (1-inputs)).sum()\n        \n        Tversky = (TP + smooth) / (TP + alpha*FP + beta*FN + smooth)  \n        FocalTversky = (1 - Tversky)**gamma\n                       \n        return FocalTversky\n\nclass myLoss(torch.nn.Module):\n\n    def __init__(self, pos_weight=1):\n      super().__init__()\n      self.pos_weight = pos_weight\n\n    def forward(self, input, target):\n      epsilon = 10 ** -44\n      input = input.sigmoid().clamp(epsilon, 1 - epsilon)\n\n      my_bce_loss = -1 * (self.pos_weight * target * torch.log(input)\n                          + (1 - target) * torch.log(1 - input))\n      add_loss = (target - 0.5) ** 2 * 4\n      mean_loss = (my_bce_loss * add_loss).mean()\n      return mean_loss\n    \n \n\nclass FocalLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(FocalLoss, self).__init__()\n\n    def forward(self, inputs, targets, alpha=ALPHA, gamma=GAMMA, smooth=1):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        #first compute binary cross-entropy \n        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n        BCE_EXP = torch.exp(-BCE)\n        focal_loss = alpha * (1-BCE_EXP)**gamma * BCE\n                       \n        return focal_loss\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from catalyst.contrib.nn import DiceLoss, IoULoss\nfrom catalyst.dl.callbacks import DiceCallback, IouCallback, \\\n  CriterionCallback, MetricAggregationCallback\n# we have multiple criterions\ncriterion = {\n    \"dice\": DiceLoss(),\n    \"iou\": IoULoss(),\n    \"bce\": nn.BCEWithLogitsLoss()\n}\n\n\ncallbacks = [\n    # Each criterion is calculated separately.\n    CriterionCallback(\n        input_key=\"mask\",\n        prefix=\"loss_dice\",\n        criterion_key=\"dice\"\n    ),\n    CriterionCallback(\n        input_key=\"mask\",\n        prefix=\"loss_iou\",\n        criterion_key=\"iou\"\n    ),\n    CriterionCallback(\n        input_key=\"mask\",\n        prefix=\"loss_bce\",\n        criterion_key=\"bce\"\n    ),\n\n    # And only then we aggregate everything into one loss.\n    MetricAggregationCallback(\n        prefix=\"loss\",\n        mode=\"weighted_sum\", # can be \"sum\", \"weighted_sum\" or \"mean\"\n        # because we want weighted sum, we need to add scale for each loss\n        metrics={\"loss_dice\": 1.0, \"loss_iou\": 1.0, \"loss_bce\": 0.8},\n    ),\n\n    # metrics\n    DiceCallback(input_key=\"mask\"),\n    IouCallback(input_key=\"mask\"),\n]\n\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from catalyst.contrib.models.cv import Unet,ResnetUnet\n#from catalyst.contrib.nn import DiceLoss, IoULoss, NLLLoss\n\nfrom catalyst.dl import SupervisedRunner\n\n# experiment setup\nnum_epochs =EPOCH\nlogdir = \"segmentation_notebook\"\n\n# model, criterion, optimizer\n#model =  torchvision.models.segmentation.fcn_resnet50(pretrained=False, progress=True, num_classes=num_class)\n#model =Unet(num_classes=num_class, in_channels=4, num_channels=64, num_blocks=4)# catalyst Unet\n#model = ResnetUnet(num_classes=num_class) # can only take 3 channels \nmodel = smp.Unet(classes=num_class, in_channels=4, activation = None)\n \ncriterion = FocalTverskyLoss()\n#\"sigmoid\" nn.BCELoss()\n#smp.utils.losses.DiceLoss()\n#WithLogits \n#myLoss()\n#nn.BCEWithLogitsLoss() # this and myLoss() can run but gradient does not descent. \n# nn.CrossEntropyLoss()  # the type should be Long\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 20, 40], gamma=0.3)\n\n# model runner\nrunner = SupervisedRunner()\n\n# model training\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    loaders=loaders,\n    #callbacks=callbacks,\n    logdir=logdir,\n    num_epochs=num_epochs,\n    verbose=True,\n    main_metric='iou',\n    callbacks=[ \n        IouCallback(),\n    ],\n    load_best_on_end=True,\n)\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#import inspect\n#inspect.signature(runner.train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f\"{logdir}//checkpoints/best_full.pth\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" \n#torch.load(f\"{logdir}//checkpoints/best_full.pth\")  # Choose whatever GPU device number you want\n ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sigmoid = lambda x: 1/(1 + np.exp(-x))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"runner_out = runner.predict_loader(loader=loaders[\"valid\"])\n\nfor i, output in enumerate (runner_out):   \n    if i == 0: \n        \n        output =  output[\"logits\"].cpu() \n        output = (output> .5).detach().numpy().astype(int)\n        for j in range (bs):\n            #print(f\"{i} th batch {j} th image\")\n            \n            plt.figure(figsize=(10,10))\n            output2 = np.sum(output[j,], axis = 0)\n       \n            plt.subplot(1 ,5, 1 )\n            \n            plt.imshow(output2, cmap=plt.get_cmap('Set3'))\n            plt.title('prediction')\n            plt.legend(f\"{j}\")\n            plt.subplot(1 ,5, 2 )\n            plt.imshow(validation[j,2,])\n            plt.title('R-band') \n            plt.subplot(1 ,5, 3 )\n            plt.imshow(validation[j,1,])\n            plt.title('NIR-band') \n            plt.subplot(1 ,5, 4)\n            plt.imshow(validation[j,0,])\n            plt.title('LIDAR-band') \n            plt.subplot(1 ,5, 5)\n            plt.imshow(y_val[j,]) \n            plt.title('validation')\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wc = 4\nrunner_out = runner.predict_loader(loader=loaders[\"valid\"])\n\nfor i, output in enumerate (runner_out):   \n    if i == 3: \n        \n        output =  output[\"logits\"].cpu() \n        output = (output > .8).detach().numpy().astype(int)\n        for j in range (bs):\n            #print(f\"{i} th batch {j} th image\")\n            \n            plt.figure(figsize=(10,10))\n            \n            output2 = output[j, wc, ]#np.sum(output[j,], axis = 0).astype(bool).astype(int)\n       \n            plt.subplot(1 ,5, 1 )\n            \n            plt.imshow(output2)\n            plt.title(f'{wc}-th class pred')\n            plt.legend(f\"{j}\")\n            plt.subplot(1 ,5, 2 )\n            plt.imshow(validation[i*16+j,2,])\n            plt.title('R-band') \n            plt.subplot(1 ,5, 3 )\n            plt.imshow(validation[i*16+j,1,])\n            plt.title('NIR-band') \n            plt.subplot(1 ,5, 4)\n            plt.imshow(validation[i*16+j,0,])\n            plt.title('LIDAR-band') \n            plt.subplot(1 ,5, 5)\n            plt.imshow(y_val[i*16+j,]) \n            plt.title('validation')\n            plt.legend()\n\n            plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classification_tile128 = gdal_array.LoadFile('../input/guided-research/Validation/class_main_12_8.tif')\nclassification_tile128=classification_tile128[::reduce,::reduce]\nnumimag = np.sqrt(validation.shape[0]/2).astype(int)\nnum = (numimag*numimag+1).astype(int)\n#fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8,8)) \n#for ii,axi in enumerate(ax.flat):\nclasses = [1,2,4,5,6,7,8,9,10,15]\nfor wc in range (10) :\n    runner_out = runner.predict_loader(loader=loaders[\"valid\"])\n    #for the first tile only\n    result = np.zeros((1,size, size))\n    for i, output in enumerate (runner_out):\n            output1 =  output[\"logits\"].cpu()\n            output1 = (output1   > .5).detach().numpy().astype(int)\n            p1 = output1[:,wc,] #np.sum(output1[0,], axis = 0).astype(bool).astype(int)    \n\n            result = np.concatenate((result, p1),axis=0)\n           # print(result.shape)     \n    result = result[1:num,:,:]\n\n    result= np.moveaxis(result, 0, -1)\n\n    big = np.zeros((numimag*size, numimag*size))\n\n    for j in range(numimag):    \n        for i in range(numimag):\n            big[j*size: (j+1)*size,i*size: (i+1)*size]= result[:,:,i+j*numimag]\n\n\n    plt.subplot(1,2,1)\n    plt.title('prediction')\n    plt.imshow(big)\n    plt.subplot(1,2,2)\n    plt.imshow(classification_tile128==classes[wc])\n    plt.title(f'validation class: {classes[wc]}')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classification_tile128 = gdal_array.LoadFile('../input/guided-research/Validation/class_main_14_7.tif')\nclassification_tile128=classification_tile128[::reduce,::reduce] \nfor wc in range (10) :\n    runner_out = runner.predict_loader(loader=loaders[\"valid\"])\n    #for the first tile only\n    result = np.zeros((1,size, size))\n    for i, output in enumerate (runner_out):\n            output1 =  output[\"logits\"].cpu()\n            output1 = (output1   > .5).detach().numpy().astype(int)\n            p1 = output1[:,wc,] #np.sum(output1[0,], axis = 0).astype(bool).astype(int)    \n\n            result = np.concatenate((result, p1),axis=0)\n           # print(result.shape)     \n    result = result[num:,:,:]\n\n    result= np.moveaxis(result, 0, -1)\n\n    big = np.zeros((numimag*size, numimag*size))\n\n    for j in range(numimag):    \n        for i in range(numimag):\n            big[j*size: (j+1)*size,i*size: (i+1)*size]= result[:,:,i+j*numimag]\n\n\n    plt.subplot(1,2,1)\n    plt.title('prediction')\n    plt.imshow(big)\n    plt.subplot(1,2,2)\n    plt.imshow(classification_tile128==classes[wc])\n    plt.title(f'validation class: {classes[wc]}')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\ntorch.save(model.state_dict(),'model.pt')\nfor wc in range (10) :\n    def extra(i,data):\n        if i == 0:\n            d = data[15,wc,]#np.sum(data[15,],axis=0).astype(bool).astype(int)\n        else:\n            d=[]\n            for x in range(15-i,16):\n                d.append(data[x,wc,])\n            d = np.concatenate(d,axis=1)\n        return d      \n\n    runner_out = runner.predict_loader(loader=loaders[\"valid\"])\n    number=1\n    a= np.empty([])\n    tot=np.empty((128*2,1920*2))\n    xx =0\n    #for the first tile only\n    for i, output in enumerate (runner_out):\n        if i in range(0,15):#batches\n            #print(f\"{i} th batch\")\n            output1 =  output[\"logits\"].cpu()\n            output1 = (sigmoid(output1)  > .7).detach().numpy().astype(int)\n            p1 = output1[0,wc,] #np.sum(output1[0,], axis = 0).astype(bool).astype(int)\n            \n            number+=1\n            if xx>0 and xx!=14:\n                p1 = np.append(next_row,p1,axis=1)\n                number+=(1*(xx+1))\n            if xx == 14:\n                p1 = np.append(next_row,p1,axis=1)\n                number+=1\n            else:\n                for j in range (1,15-xx):#images in 1 batch\n                    output2 = output1[j,wc,]#np.sum(output1[j,], axis = 0).astype(bool).astype(int)\n                    p1=np.append(p1,output2,axis=1)\n                    number+=1\n                next_row = extra(xx,output1)\n                xx+=1\n        else:\n            continue\n        tot = np.append(tot,p1,axis=0)\n\n        #print('tot',tot.shape,number)\n\n\n\n    tot=tot[128*2:,:]\n    #print(tot.shape[0]/128,tot.shape[1]/128,number)  \n    plt.imshow(tot)\n    plt.title(f'class: {wc}')\n    plt.show()\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"''' # probabilistic\nrunner_out = runner.predict_loader(loader=loaders[\"valid\"])\nwh = 2\nplt.figure(figsize=(18,18))\nfor i, output in enumerate (runner_out):   \n    if i == 0: \n        print(i)\n        for j in range(1,10): \n            \n            plt.subplot(10,3, 1+3*(j-1))\n            plt.imshow(output['logits'].cpu()[wh,j,])\n            plt.subplot(10,3, 2+3*(j-1))\n            plt.imshow(validation[wh,2,])\n            plt.subplot(10, 3, 3+3*(j-1))\n            plt.imshow(new_mas_val[wh,j,]) \n''' \n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n''' plot\n#plt.style.use(\"ggplot\")\n#%matplotlib inline\n\nsigmoid = lambda x: 1/(1 + np.exp(-x))\n\nfor i, (input, output) in enumerate(zip(validation, runner_out)):\n    image, mask = input\n    \n    threshold = 0.5\n    \n    plt.figure(figsize=(10,8))\n    \n    \n    plt.imshow(image, 'gray')\n    \n    plt.subplot(1, 3, 2)\n    output =  output[\"logits\"].copy()\n    output = (output > threshold).astype(np.uint8)\n    plt.imshow(output, \"gray\")\n    \n    plt.subplot(1, 3, 3)\n    plt.imshow(mask, \"gray\")\n    \n    plt.show()\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How to update a model\nhttps://heartbeat.fritz.ai/image-segmentation-with-transfer-learning-pytorch-5ada7121c6ab"},{"metadata":{"trusted":true},"cell_type":"code","source":"'''#for loop way of class2dimm\ndef class2dim2 (mask, num_class):\n    \n    new_mask = np.reshape(np.tile(mask, num_class), ( mask.shape[0], mask.shape[1],mask.shape[2], num_class))\n \n    for j in range(mask.shape[0]):\n         aslice = new_mask[j,]\n         amask = mask[j,]\n         for i in range(num_class):\n             aslice[:,:,i] = np.multiply(amask == i+1,1)        \n         new_mask[j,]=aslice\n    return new_mask\n    \na = class2dim2(y_train, num_class)\na.shape\ncomparison = a == new_mask\nequal_arrays = comparison.all()\nprint(equal_arrays)\n'''\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"''' # Not using catalyst, the lost is a different format \n\nfrom torch.utils.data import DataLoader, TensorDataset \ndef myloader(trainX, trainY, valX,valY):\n    train = TensorDataset(torch.Tensor(trainX[:,0:3,:,:]), torch.Tensor(trainY )) # create your datset\n    train  = DataLoader(train, batch_size=bs) # create your dataloader\n    \n    vali= TensorDataset(torch.Tensor(valX[:,0:3,:,:]),torch.Tensor(valY  )) # create your datset\n    vali = DataLoader(vali, batch_size=bs) # create your dataloader\n    return train , vali \n\ntrain_loader, valid_loader = myloader(training, new_mask, validation, new_mas_val)\n # torch.LongTensor() for some losses\nCLASSES = [format(x, '2d') for x in  classes]\n \nmodel = smp.Unet(classes=len(CLASSES), in_channels=3,activation=\"sigmoid\")\n#newmodel = torch.nn.Sequential(*(list(model.children())[:-1]))\n#print(model)\n\nENCODER = 'se_resnext50_32x4d'\nENCODER_WEIGHTS = 'imagenet'\n\nprint(len(CLASSES))\nACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multicalss segmentation\nDEVICE = 'cuda'\n\n# create segmentation model with pretrained encoder\n#model = smp.FPN(\n#    encoder_name=ENCODER, \n#    encoder_weights=ENCODER_WEIGHTS, \n#    classes=len(CLASSES), \n#    activation=ACTIVATION,\n#)\n\npreprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n\nloss = smp.utils.losses.DiceLoss()\n#loss=nn.BCELoss(reduction = 'sum')\n#WithLogits\n#loss = nn.BCELoss()\nmetrics = [\n    smp.utils.metrics.IoU(threshold=0.5),\n]\n\noptimizer = torch.optim.Adam([ \n    dict(params=model.parameters(), lr=0.0001),\n])\ntrain_epoch = smp.utils.train.TrainEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    optimizer=optimizer,\n    device=DEVICE,\n    verbose=True,\n)\n\nvalid_epoch = smp.utils.train.ValidEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    device=DEVICE,\n    verbose=True,\n)\nmax_score = 0\n\nfor i in range(0, EPOCH):\n    \n    print('\\nEpoch: {}'.format(i))\n    train_logs = train_epoch.run(train_loader)\n    valid_logs = valid_epoch.run(valid_loader)\n    \n    # do something (save model, change lr, etc.)\n    if max_score < valid_logs['iou_score']:\n        max_score = valid_logs['iou_score']\n        torch.save(model, 'best_model.pth')\n        print('Model saved!')\n        \n    if i == 25:\n        optimizer.param_groups[0]['lr'] = 1e-5\n\n        print('Decrease decoder learning rate to 1e-5!')\n\nbest_model = torch.load('./best_model.pth')\ndef visualize(**images):\n    \"\"\"PLot images in one row.\"\"\"\n    n = len(images)\n    plt.figure(figsize=(16, 5))\n    for i, (name, image) in enumerate(images.items()):\n        plt.subplot(1, n, i + 1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(' '.join(name.split('_')).title())\n        plt.imshow(image)\n    plt.show()\n    \nfor i in range(10):\n    x_tensor = torch.from_numpy(validation[:,0:3,:,:]).to(DEVICE)     \n    pr_mask = best_model.predict(x_tensor)\n    pr_mask = (pr_mask.squeeze().cpu().numpy())\n        \n    visualize( \n        predicted_mask=pr_mask[2,i,:,:]\n    )\nplt.imshow(validation[2,1,:,:])\nplt.show()\nplt.imshow(y_val[2,:,:])\n''' ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}